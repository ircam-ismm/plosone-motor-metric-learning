import logging

import numpy as np
np.set_printoptions(formatter={'float': '{: 0.3f}'.format})
import pandas as pd
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.model_selection import RepeatedKFold
import sklearn.model_selection as skms

import sklearn.preprocessing as skprep
import scipy.optimize as sopt

import fastdtw
import fastdtw2

import mlflow
from mlflow.utils.file_utils import TempDir

import pingouin as pg

from metric_learning.extras import utils


def ponderate(arr, weights):
    """Create an array of shape arr.shape[0] with weights.shape[0] equi-spaced
    segments of values weights.
    """
    n_weights = weights.shape[0]
    weights = weights.reshape(-1,1)
    region_size = arr.shape[0] // n_weights
    remainder = arr.shape[0] % n_weights
    vec = np.hstack([(weights * np.ones((n_weights, region_size))).reshape(-1),
                     weights[-1] * np.ones(remainder)])
    return vec


from sklearn.linear_model import LinearRegression
from scipy.optimize import curve_fit
def sigmoid_1(x, a, b):
    return a * 1/(1 + np.exp(-b * x)) - 1


class LR(LinearRegression):

    def rmse(self, X, y):
        y_true = y
        y_pred = self.predict(X)
        res = ((y_true - y_pred) ** 2).sum()
        return res / X.shape[0]

class Sigmoid():
    popt = None

    def _sigmoid(self, x, a, b):
        return a * 1/(1 + np.exp(-b * x)) - 1

    def fit(self, X, y):
        popt, pcov = curve_fit(self._sigmoid, X.reshape(-1), y.reshape(-1))
        self.popt = popt
        return self

    def predict(self, X):
        if self.popt is None:
            raise("Fit model before predict.")
        return self._sigmoid(X, *self.popt)

    def rmse(self, X, y):
        y_true = y
        y_pred = self.predict(X)
        res = ((y_true - y_pred) ** 2).sum()
        return res / X.shape[0]


class StopOptimizingException(Exception):
    """Raised in scipy minimize to stop the optimisation.
    """
    pass


class Model(BaseEstimator, RegressorMixin):
    """Regression model for between human annotations and DTW path scores.
    This particular version can optmise n_regions segments on the path score.
    The data is stored in one singleton and the compute is shared across process.
    That is X stores the key to access the data.

    Args:
        users ([type]): [description]
        parameters (dict): Parameters for X, Y, Z
    """
    def __init__(self,
        users,
        templates,
        n_regions,
        init_weights,
        parameters) -> None:
        super().__init__()

        self.log = logging.getLogger(__name__)

        self.parameters = parameters
        self.users = users
        self.templates = templates

        self.n_regions = n_regions
        self.init_weights = init_weights
        self.weights = init_weights
        self.dimensions = parameters['data']['dims']
        self.dtw_radius = parameters['dtw']['radius']
        self.dtw_fun = parameters['dtw']['fun']



    def fit(self, X, y):
        self.step = 0
        self.last_cc_valid = -1

        def cb(xk):
            cc_valid = self.score(X, y, theta=xk)
            self.log.info(xk)
            self.log.info("CV[{}]: {:.3f} {:.3f}".format(self.step, self.last_cc_valid, cc_valid))

            mlflow.log_metric(key="score", value=cc_valid, step=self.step)

            # store the latest weights
            self.weights = xk
            self.step += 1

        X_init = self.init_weights
        n_iter = self.parameters['opt']['n_iter']
        optim_options = {'disp': None, 'gtol': 1e-36, 'eps': 1e-3, 'maxiter': n_iter, }
        optim_bounds = [(1e-5*f,1e5*f) for f in np.ones(self.n_regions)]

        def loss(xk, X, y):
            loss = np.abs(1 - self.score(X, y, xk))
            return loss

        try:
            res = sopt.minimize(loss, X_init, args=(X, y, ),
                                method='L-BFGS-B', callback=cb,
                                options=optim_options,
                                bounds=optim_bounds)

            print("SCIPY RES:", res)
        except StopOptimizingException:
            pass



    def fit_with_validation(self, Xtrain, ytrain, Xvalid, yvalid):
        """This fits the modedtw_l until the validation performance disminishes."""

        self.step = 0
        self.last_cc_valid = -1

        cc_train = self.score(Xtrain, ytrain)

        def cb(xk):
            cc_valid = self.score(Xtrain, ytrain, theta=xk)
            # cc_valid = self.score(Xvalid, yvalid, theta=xk)
            self.log.info("CV[{}]: {:.3f} {:.3f}".format(self.step, self.last_cc_valid, cc_valid))

            # early stopping condition
            if np.isclose(self.last_cc_valid, cc_valid, atol=1e-03):
                self.log.info("EARLY STOPPING: {}".format(self.step))
                raise StopOptimizingException()
            # store the latest weights
            else:
                self.last_cc_valid = cc_valid
                self.weights = xk

            self.step += 1

        X_init = np.ones(self.n_regions)
        n_iter = self.parameters['opt']['n_iter']
        optim_options = {'disp': None, 'gtol': 1e-36, 'eps': 1e-3, 'maxiter': n_iter, }
        optim_bounds = [(1e-5*f,1e5*f) for f in np.ones(self.n_regions)]

        def loss(xk, X, y):
            loss = self.score(X, y, xk)
            reg = np.linalg.norm(1 - self.weights) + 1e-2# / self.weights.shape[0]
            return loss + reg

        try:
            res = sopt.minimize(loss, X_init, args=(Xtrain, ytrain, ),
                                method='L-BFGS-B', callback=cb,
                                options=optim_options,
                                bounds=optim_bounds)
        except StopOptimizingException:
            pass


    def compute(self, X, theta=None) -> float:
        """Score X for target y, with model saved weights or given input weights
        theta."""
        if theta is None:
            theta = self.weights

        # compute
        res = X.apply(self._compute_dtw_row, args=(theta, ), axis=1)
        return res


    def score(self, X, y, theta=None, fun='rmse') -> float:
        """Score X for target y, with model saved weights or given input weights
        theta."""

        res = self.compute(X, theta)

        if fun =='rmse':
            X = np.array(res).reshape(-1,1)
            y = y.values.reshape(-1,1)
            self.lr = LR().fit(X, y)
            score = self.lr.rmse(X, y)

        if fun == 'correlation':
            corrcoef = np.corrcoef(y.values, np.array(res))[0,1]
            score = corrcoef

        return score


    def _compute_dtw_row(self, row, weights):
        """Compute the DTW between both observations described in the row and
        the template. Return the difference between the DTW.
        """
        # select data and normalise
        a, _, path_a, t = self._cached_compute(row['user'], row['day_0'], row['rep_0'])
        b, _, path_b, t = self._cached_compute(row['user'], row['day_1'], row['rep_1'])

        # ponderate
        vec = ponderate(t, np.arange(weights.shape[0]))

        da_ = np.linalg.norm( (a[path_a[:,0]] - t[path_a[:,1]]) , axis=1, ord=1)
        df = pd.DataFrame(data=np.vstack([vec[path_a[:, 1]], da_]).T, columns=['s', 'd']).astype({"s":int})
        da_ = (df.groupby("s").agg(self.dtw_fun)['d'] * weights).sum()

        db_ = np.linalg.norm( (b[path_b[:,0]] - t[path_b[:,1]]) , axis=1, ord=1)
        df = pd.DataFrame(data=np.vstack([vec[path_b[:, 1]], db_]).T, columns=['s', 'd']).astype({"s":int})
        db_ = (df.groupby("s").agg(self.dtw_fun)['d'] * weights).sum()

        return da_ - db_


    @utils.memoized_method(maxsize=1024)
    def _cached_compute(self, user, day, trial):
        """Cached compute of the DTW between a motion and a template.
        """
        a = utils.select(self.users, gesture=1, user=user, day=day, trial=trial)
        a = skprep.StandardScaler().fit_transform(a[list(self.dimensions)])

        t = utils.select(self.templates, template=1, version=0)
        t = skprep.StandardScaler().fit_transform(t[list(self.dimensions)])

        da, path_a = fastdtw.fastdtw(a, t, radius=self.dtw_radius)
        path_a = np.array(path_a)
        return a, da, path_a, t


def train_test_split(annotations, train_ids, test_ids):
    """Implement the same functionality as sklearn, but with our data. This uses
    the user id in the dataframe to select folds that are split by users.
    """
    X = annotations[['user', 'day_0', 'rep_0', 'day_1', 'rep_1']]
    y = annotations[['user', 'm']]

    users_ids = np.array(list(set(annotations['user'])))

    Xtrain = utils.select(X, user=list(users_ids[train_ids]))
    ytrain = utils.select(y, user=list(users_ids[train_ids]))['m']
    Xtest = utils.select(X, user=list(users_ids[test_ids]))
    ytest = utils.select(y, user=list(users_ids[test_ids]))['m']

    return Xtrain, ytrain, Xtest, ytest


def main_with_cv(users, templates, annotations, parameters):
    """Loop over regions and cross validates training with n-repeated k-folds.
    """

    # hyper parameter search
    n_regions = parameters['opt']['n_regions']
    for n_region in n_regions:

        logging.info("start of n_region: {}".format(n_region))

        columns = ['fold_id', 'train_ids', 'test_ids', 'cc_start', 'cc_end']+['w'+str(i) for i in range(n_region)]
        report = pd.DataFrame(columns=columns)

        scale_init = parameters['opt']['init_scale']
        X_init = np.ones(n_region) * scale_init

        model = Model(users, templates, n_region, X_init, parameters)

        experiment_id = parameters['mlflow']['experiment_id']

        with mlflow.start_run(experiment_id=experiment_id):
            mlflow.log_params(utils.flatten(parameters))
            mlflow.log_param("n_region", n_region)

            # cross validate the experiment
            n_splits = parameters['cv']['n_splits']
            n_repeats = parameters['cv']['n_repeats']
            rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)
            users_ids = np.array(list(set(annotations['user'])))
            for fold_id, (train_ids, test_ids) in enumerate(rkf.split(users_ids)):

                # reset model weights between folds
                model.weights = model.init_weights

                # save test data for performance measure
                Xtrain2, ytrain2, Xtest, ytest = train_test_split(annotations, train_ids, test_ids)
                # train2_ids, valid_ids = skms.train_test_split(train_ids, test_size=0.5, random_state=42)
                # Xtrain2, ytrain2, Xvalid, yvalid = train_test_split(annotations, train2_ids, valid_ids)
                logging.info("LOOP {}: {}".format(fold_id, (train_ids, True, test_ids)))

                # baseline on test
                cc_start = model.score(Xtest, ytest, fun='correlation')

                # train the model
                model.fit_with_validation(Xtrain2, ytrain2, True, True)

                # final performance on test
                cc_end = model.score(Xtest, ytest, fun='correlation')
                # report for current fold
                row = pd.DataFrame(data=[[fold_id, train_ids, test_ids, cc_start, cc_end]+list(model.weights)],
                                columns=columns)

                logging.info("LOOP {}: {:.3f} -> {:.3f}".format(fold_id, cc_start, cc_end))
                report = report.append(row)


            # score model on weights' average and full dataset
            X, y = annotations[['user', 'day_0', 'rep_0', 'day_1', 'rep_1']], annotations['m']
            model.weights = model.init_weights
            cc_init = model.score(X, y)
            mlflow.log_metric(key="cc_init", value=cc_init)

            weights = report.filter(regex="w.*").mean().values
            model.weights = weights
            cc_all = model.score(X, y)
            mlflow.log_metric(key="cc_all", value=cc_all)

            # log the relative correlation change
            mlflow.log_metric(key="cc_end", value=report['cc_end'].mean())
            mlflow.log_metric(key="cc_start", value=report['cc_start'].mean())
            diff_rel = (report['cc_end'].mean() - report['cc_start'].mean()) / report['cc_start'].mean()
            mlflow.log_metric(key="diff_rel", value=diff_rel)

            # ttest on final performance change
            ttest = pg.ttest(report['cc_start'], report['cc_end'])
            mlflow.log_metric(key="p-value", value=ttest['p-val'].values[0])
            mlflow.log_metric(key="power", value=ttest['power'].values[0])

            # log artifacts
            with TempDir() as tmp:
                file_path = tmp.path("report.csv")
                with open(file_path, 'w') as f: report.to_csv(f)
                mlflow.log_artifact(file_path)

            with TempDir() as tmp:
                file_path = tmp.path("params.txt")
                import json
                with open(file_path, 'w') as f: json.dump(parameters, f)
                mlflow.log_artifact(file_path)

            with TempDir() as tmp:
                file_path = tmp.path("src.txt")
                import inspect
                import sys
                data = inspect.getsource(sys.modules[__name__])
                with open(file_path, 'w') as f:
                    for line in data: f.write(line)
                mlflow.log_artifact(file_path)


    return report, parameters


def main_no_cv(users, templates, annotations, parameters):
    """This is meant to optimise full on one dataset and test on the other one.
    The two dataset used here are individual and mean.
    """

    # hyper parameter search
    n_regions = parameters['opt']['n_regions']
    for n_region in n_regions:

        logging.info("start of n_region: {}".format(n_region))

        scale_init = parameters['opt']['init_scale']
        X_init = np.ones(n_region) * scale_init

        model = Model(users, templates, n_region, X_init, parameters)

        experiment_id = parameters['mlflow']['experiment_id']

        X, Y = annotations.iloc[:90], annotations.iloc[90:]
        Xtrain, ytrain = X[['user', 'day_0', 'rep_0', 'day_1', 'rep_1']], X['m']
        Xtest, ytest = Y[['user', 'day_0', 'rep_0', 'day_1', 'rep_1']], Y['m']

        with mlflow.start_run(experiment_id=experiment_id):
            mlflow.log_params(utils.flatten(parameters))
            mlflow.log_param("n_region", n_region)

            # baseline on test
            cc_start = model.score(Xtest, ytest, refit_model=True)
            logging.info("SCORED: {}".format(cc_start))

            # train the model
            model.fit(Xtrain, ytrain)

            # final performance on test
            cc_end = model.score(Xtest, ytest)
            logging.info("FITTED: {}".format(cc_end))


    return False, parameters


from metric_learning.pipelines import select_annotations


def main(users, templates, annotations, parameters):

    if parameters["cv"]['active']:
        A, B = main_with_cv(users, templates, annotations, parameters)

    if not parameters["cv"]['active']:
        A, B = main_no_cv(users, templates, annotations, parameters)

    return A, B


from kedro.pipeline import Pipeline, node
def create_pipeline(**kwargs):
    return Pipeline(
        [
            node(
                func=select_annotations,
                inputs="parameters",
                outputs="annotations",
                name="select_annotations_node"
            ),

            node(
                func=main,
                inputs=["users", "templates", "annotations", "parameters"],
                outputs=["report", "config"],
                name="temporal_correlation_node"
            ),
        ],
        tags="compute"
    )





# vec_a = vec[path_a[:, 1]].reshape(-1,1)
# vec_b = vec[path_b[:, 1]].reshape(-1,1)
# weights_maha = np.array([ 1.014,  0.349,  1.756,  1.214,  0.587,  0.535,  1.499,  0.574,  1.140,  2.080,
# 0.202,  0.527,  1.217, -0.135,  0.721,  0.694,  0.799, -0.028, -0.261,  0.046,
# 1.073, -0.201, -0.322, -0.096, -0.365,  0.458, -1.222, -0.214, -0.255, -0.080,
# -0.524,  0.247, -0.523,  0.362,  0.176, -0.073,  0.643, -0.766,  2.559,  0.357,
# 0.608,  0.226,  1.120, -0.858,  1.028, -0.856, -0.401,  0.170,  0.567,  0.234,
# -0.177,  1.617, -0.584, -0.081, -1.351,  0.097, -0.086, -0.321,  0.315,  0.122,
# 0.484, -0.046, -0.015,  0.156,  0.878,  0.252, -0.401, -0.098,  0.227, -0.454,
# 0.084, -0.047, -0.201, -0.345, -0.049, -0.459,  0.237,  0.370,  0.641, -0.012,
# 0.122,  0.743,  0.188, -0.071, -0.505, -0.407, -0.292, -1.044, -0.287,  0.206,
# 1.389,  0.184,  0.417,  1.926,  0.033, -0.170, -0.012, -0.170, -0.412, -0.744,
# -0.093, -0.046,  1.037,  0.537,  0.724,  1.964, -0.149, -0.270,  1.369, -0.064,
# 0.998,  0.566,  0.468, -0.095,  0.598,  0.023,  1.859,  1.124, -0.405, -0.207,
# 0.236,  0.383, -1.396,  0.121, -0.859, -0.201,  0.168,  0.769,  0.793,  2.901,
# 0.203, -0.304, -0.037,  0.327, -0.392,  0.688, -0.454,  0.023, -0.797, -0.072,
# -0.422, -0.235,  0.393,  0.122, -0.233, -0.478,  0.817,  0.194,  0.015,  0.304,
# 0.144, -0.497, -0.324, -0.913,  0.083,  0.207])
# data_shape = len(self.dimensions)
# A = np.eye(data_shape) * weights_maha[:data_shape]
# B = weights_maha[data_shape:].reshape(data_shape, data_shape)
# M = B@A@B.T
# da_ = np.linalg.norm( vec_a*(a[path_a[:,0]] - t[path_a[:,1]]) , axis=1, ord=2).sum()
# db_ = np.linalg.norm( vec_b*(b[path_b[:,0]] - t[path_b[:,1]]) , axis=1, ord=2).sum()
# normalise each segments so that the sum is equal to 1
# def maha(x, y, ic=M):
#     # https://github.com/scipy/scipy/blob/v1.6.3/scipy/spatial/distance.py#L1049-L1093
#     delta = x - y
#     m = np.dot(np.dot(delta, ic), delta)
#     return np.sqrt(m)
# da_ = np.array([maha(x, y) for (x, y) in zip(a[path_a[:,0]], t[path_a[:,1]])])
